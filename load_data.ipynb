{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "albums = [\"Taylor Swift\", \"Fearless (Taylor's Version)\", \"Speak Now (Taylor's Version)\", \n",
    "          \"Red (Taylor's Version)\", \"1989 (Taylor's Version)\", \"reputation\", \"Lover\", \n",
    "          \"Midnights\", \"folklore\", \"evermore\", \"The Tortured Poets Department\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('songs.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_album = df[df['Album'].isin(albums)]\n",
    "df = in_album"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Title                          Album  \\\n",
      "0                                Tim McGraw                   Taylor Swift   \n",
      "1                           Picture to Burn                   Taylor Swift   \n",
      "2                    Teardrops On My Guitar                   Taylor Swift   \n",
      "3                     A Place In This World                   Taylor Swift   \n",
      "4                               Cold as You                   Taylor Swift   \n",
      "..                                      ...                            ...   \n",
      "233                          The Manuscript  The Tortured Poets Department   \n",
      "252    Bad Blood (Remix) (Taylor's Version)        1989 (Taylor's Version)   \n",
      "253                                   Paris                      Midnights   \n",
      "254                                  Glitch                      Midnights   \n",
      "256  If This Was a Movie (Taylor's Version)    Fearless (Taylor's Version)   \n",
      "\n",
      "                                                Lyrics  \n",
      "0    He said the way my blue eyes shined\\nPut those...  \n",
      "1    State the obvious, I didn't get my perfect fan...  \n",
      "2    Drew looks at me\\nI fake a smile so he won't s...  \n",
      "3    I don't know what I want, so don't ask me\\n'Ca...  \n",
      "4    You have a way of coming easily to me\\nAnd whe...  \n",
      "..                                                 ...  \n",
      "233  Now and then, she rereads the manuscript\\nOf t...  \n",
      "252  'Cause, baby, now we've got bad blood\\nYou kno...  \n",
      "253  \"Your ex-friend's sister\\nMet someone at a clu...  \n",
      "254  We were supposed to be just friends\\nYou don't...  \n",
      "256  Last night, I heard my own heart beatin'\\nSoun...  \n",
      "\n",
      "[236 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "within_description = 0\n",
    "for lyric in df['Lyrics']:\n",
    "    word_list=lyric.split()\n",
    "    word_data = \"\"\n",
    "    for word in word_list:\n",
    "        if \"[\" or \"]\" in word:\n",
    "            if \"[\" in word and not(\"]\" in word):\n",
    "                within_description = 1\n",
    "            elif \"]\" in word and not(\"[\" in word):\n",
    "                within_description = 0\n",
    "            elif \"[\" in word and \"]\" in word:\n",
    "                within_description = 0\n",
    "            if \"(\" in word and not(\")\" in word):\n",
    "                within_description = 1\n",
    "            elif \")\" in word and not(\"(\" in word):\n",
    "                within_description = 0\n",
    "            elif \"(\" in word and \")\" in word:\n",
    "                within_description = 0\n",
    "        if not(\"]\" in word or \"[\" in word or \"(\" in word or \")\" in word) and not(within_description):\n",
    "            for contraction in contractions.contractions_dict.keys():\n",
    "                if contraction.lower() in word.lower():\n",
    "                    word = contractions.fix(word)\n",
    "            word_data += word + \" \"\n",
    "\n",
    "    nltk_tokens = nltk.word_tokenize(text=word_data, language='english', preserve_line=True)\n",
    "\n",
    "    pronouns_and_not = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', \n",
    "                'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', \n",
    "                'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'not']\n",
    "    \n",
    "    stop_words = [word for word in stop_words if word not in pronouns_and_not]\n",
    "\n",
    "    nltk_tokens = [token.lower() for token in nltk_tokens if token.lower() not in stop_words]\n",
    "    nltk_tokens = [token for token in nltk_tokens if token not in string.punctuation]\n",
    "    nltk_tokens = [lemmatizer.lemmatize(token) if token != \"us\" else \"us\" for token in nltk_tokens ]\n",
    "    nltk_tokens = [token for token in nltk_tokens if token != \"''\" and \n",
    "                   token != \"``\" and token != \"'s\" and token != \"'d\" and token != \"oh\"\n",
    "                   and token != \"hey\" and token != \"'m\" and token != \"mm\" and token != \"oh-oh\"\n",
    "                   and token != \"oh-oh-oh\" and token != \"mmm\" and token != \"ooh\" and token != \"ooh-oh\"\n",
    "                   and token != \"mm-mm\" and token != \"la\" and token != \"ah-ah\" and token != \"ha\"\n",
    "                   and token != \"ha-ah\" and token != \"di\" and token != \"da\" and token != \"da-da\" and token != \"uh-huh\"\n",
    "                   and token != \"ra-di-di-di-di-di-di-di-di-di-da-da\"]\n",
    "    \n",
    "    lyric_column = df['Lyrics'] == lyric\n",
    "    row_index = df.index[lyric_column]\n",
    "    title = df[df['Lyrics'] == lyric]['Title'].values[0]\n",
    "    album = df[df['Lyrics'] == lyric]['Album'].values[0]\n",
    "    file_path = 'songs_excl_ftv/'+ album\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "    song = os.path.join(file_path, f\"{title}.txt\")\n",
    "    if not(\"[From the Vault]\" in title or \"[From The Vault]\" in title or \"(Remix)\" in title):\n",
    "        with open(song, \"w\", encoding=\"utf-8\") as f:\n",
    "            literal = \"\\n\".join(nltk_tokens)\n",
    "            f.write(literal)\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosi114a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
